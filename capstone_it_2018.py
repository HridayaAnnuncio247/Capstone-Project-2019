# -*- coding: utf-8 -*-
"""Capstone_IT_2018

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rAzR5GBGv1isbWb785SCEYGWvNLoMUQE
"""

import numpy as np
import pandas as pd
from google.colab import files

uploaded= files.upload()

d = pd.read_csv("combined_IT.csv")
d.head()

d1 = d.iloc[:, 4:5]
d1.shape

x_train = d.iloc[0:200, 4:5]
x_test = d.iloc[200:248, 4:5]
x_test.head()

print(x_train.shape,x_test.shape)

"""**Null Hypothesis: series is non-stationary**"""

from statsmodels.tsa.stattools import adfuller
from numpy import log
result = adfuller(x_train['Close'])
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])

"""Accept null hypothesis as p-value>0.05

```
# This is formatted as code
```

**Finding differencing factor:**
"""

result = adfuller(x_train['Close'].diff().dropna())
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])

"""Reject null hypothesis in first differnce itself, hence stationary!"""

''''import numpy as np, pandas as pd
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import matplotlib.pyplot as plt

# Original Series
fig, axes = plt.subplots(3, 2, sharex=True)
axes[0, 0].plot(x_train.value); axes[0, 0].set_title('Original Series')
plot_acf(x_train['Close'], ax=axes[0, 1])

# 1st Differencing
axes[1, 0].plot(x_train.value.diff()); axes[1, 0].set_title('1st Order Differencing')
plot_acf(x_train.value.diff().dropna(), ax=axes[1, 1])

# 2nd Differencing
axes[2, 0].plot(x_train.value.diff().diff()); axes[2, 0].set_title('2nd Order Differencing')
plot_acf(x_train.value.diff().diff().dropna(), ax=axes[2, 1])

plt.show()''''

"""**Finding AR factor(p):**"""

# PACF plot of 1st differenced series
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import matplotlib.pyplot as plt
plt.rcParams.update({'figure.figsize':(9,3), 'figure.dpi':120})

fig, axes = plt.subplots(1, 2, sharex=True)
axes[0].plot(x_train['Close'].diff()); axes[0].set_title('1st Differencing')
axes[1].set(ylim=(0,5))
plot_pacf(x_train['Close'].diff().dropna(), ax=axes[1])

plt.show()

"""Let p=1

**Finding no. of MA terms (q)**
"""

fig, axes = plt.subplots(1, 2, sharex=True)
axes[0].plot(x_train['Close'].diff()); axes[0].set_title('1st Differencing')
axes[1].set(ylim=(0,1.2))
plot_acf(x_train['Close'].diff().dropna(), ax=axes[1])

plt.show()

"""q=`1

**ARIMA model:**
"""

from statsmodels.tsa.arima_model import ARIMA

# 1,1,2 ARIMA Model
model = ARIMA(x_train['Close'], order=(1,1,1))
model_fit = model.fit(disp=0)
print(model_fit.summary())

model_fit.plot_predict(dynamic=False)
plt.show()

# model = ARIMA(train, order=(3,2,1))  
model = ARIMA(x_train, order=(1,1, 1))  
fitted = model.fit(disp=-1)  

# Forecast
fc, se, conf = fitted.forecast(48,alpha=0.05)  # 95% conf

# Make as pandas series
fc_series = pd.Series(fc, index=x_test.index)
lower_series = pd.Series(conf[:, 0], index=x_test.index)
upper_series = pd.Series(conf[:, 1], index=x_test.index)

# Plot
plt.figure(figsize=(12,5), dpi=100)
plt.plot(x_train, label='training')
plt.plot(x_test, label='actual')
plt.plot(fc_series, label='forecast')
plt.fill_between(lower_series.index, lower_series, upper_series, 
                 color='k', alpha=.15)
plt.title('Forecast vs Actuals')
plt.legend(loc='upper left', fontsize=8)
plt.show()

forecast, stderr, conf = fitted.forecast()
print(forecast)
#print( % x_test[0])

"""#Prophet"""

pip install fbprophet

from fbprophet import Prophet
import matplotlib.pyplot as plt

d.head()

plt.plot(d['Date'], d['Close'])
plt.ylabel('Closing Price')
plt.xlabel('Date')
plt.plot

"""#LSTM"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout

x_train1 = pd.DataFrame()
y_train1 = pd.DataFrame()
xn = pd.DataFrame()
yn= pd.DataFrame()
#

for i in range(60, 200):
  xn =  d1.iloc[i-60: i , :]
  xn = xn.transpose()
  xn.columns = range(xn.shape[1])
  #yn=xn.iloc[0:1, : ]
  #print('i:',i,'yn:',yn)
  x_train1= x_train1.append(xn.iloc[0:1 , :], ignore_index=True)
  y_train1= y_train1.append(x_train.iloc[i:i+1,0:1])

x_train1= scaler.transform(x_train1)

print(x_train1.shape,y_train1.shape)

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range = (0, 1))
x_train = scaler.fit_transform(x_train)

x_train1, y_train1 = np.array(x_train1), np.array(y_train1)
#x_train1 = np.reshape(x_train)

x_train1 = np.reshape(x_train1, (x_train1.shape[0], x_train1.shape[1], 1))

x_test1 = pd.DataFrame()
y_test1 = pd.DataFrame()

for i in range(201, 248):
  xn =  d1.iloc[i-60: i , :]
  xn = xn.transpose()
  xn.columns = range(xn.shape[1])
  #yn=xn.iloc[0:1, : ]
  #print('i:',i,'yn:',yn)
  x_test1= x_test1.append(xn.iloc[0:1 , :], ignore_index=True)
  y_test1= y_test1.append(d1.iloc[i:i+1,0:1])

 # x_test1= x_test1.append(d1.iloc[i-60:i,0:1])
 # y_test1= y_test1.append(d1.iloc[i:i+1,0:1])

print(x_test1.shape,y_test1.shape)

x_test1= np.array(x_test1)

x_test1 = np.reshape(x_test1, (x_test1.shape[0], x_test1.shape[1], 1))

x_test1

''''apple_testing_complete = pd.read_csv(r'E:\Datasets\apple_testing.csv')
apple_testing_processed = apple_testing_complete.iloc[:, 1:2].values''''

model = Sequential()

model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train1.shape[1],1))) # 50 neurons, x_train[1]-> number of timesteps, 1-> no. of indicators(closing price)
model.add(Dropout(0.2))

model.add(LSTM(units=50, return_sequences=True))
model.add(Dropout(0.2))

model.add(LSTM(units=50, return_sequences=True))
model.add(Dropout(0.2))

model.add(LSTM(units=50))
model.add(Dropout(0.2))

model.add(Dense(units = 1))

model.compile(optimizer = 'adam', loss = 'mean_squared_error')

''''x_train1 = d.iloc[0:200, 4:7]
x_test1 = d.iloc[200:248, 4:7]
x_test1.head()''''

plt.scatter(x_train1,y_train1)

model.fit(x_train1, y_train1 , epochs = 10, batch_size = 32)

y_test = model.predict(x_test1)

y_test1.shape

y_test

import sklearn.preprocessing

y_test = scaler.inverse_transform(y_test)

#y_test = scale#r.inverse_transform(y_test1)

import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))
plt.plot(y_test1, color='blue', label='Actual Closing Price')
plt.plot(y_test , color='red', label='Predicted Closing Price')
plt.title('Apple Stock Price Prediction')
plt.xlabel('Date')
plt.ylabel('Apple Stock Price')
plt.legend()
plt.show()